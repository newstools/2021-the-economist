EVERY FEW weeks from June 1963 until July 1968, Robert Paine, a zoologist, made the journey from Seattle, where he taught at the University of Washington, across Puget Sound to the rocky shores of Mukkaw bay. There, he had found virtually pristine tide pools that teamed with life—limpets, anemones, mussels, seaweeds and purple-and-orange seastars known as Pisaster ochraceus. The unspoiled landscape offered the perfect setting for what was to become a seminal experiment in ecology. On each visit, Dr Paine systematically removed all the seastars he could find from one patch of rock, lobbing them as far as he could into the waves. He did this for five years, all the while carefully documenting how the shoreline communities evolved. Very little changed in the untouched areas. But in his seastar-free zone, everything was altered. Pisaster is a greedy carnivore that feasts on mussels, barnacles, limpets and snails. Released from their predator, these species began to spread out. The acorn barnacles took over first. Later, they were displaced by goose barnacles and mussels. By removing just one species, Dr Paine had triggered a domino effect. Soon, the number of species in the community had dropped from 15 to eight. By 1968, the mussels had taken over completely. Dr Paine dubbed Pisaster a “keystone species”; remove it and the ecosystem is transformed. Large herbivores like rhinos are keystone species, spreading seeds of the plants they consume across vast areas, thus maintaining or altering vegetation. In the kelp forests of the Pacific Northwest, sea otters play a keystone role by munching on sea urchins. The urchins graze on kelp and, left unchecked, are capable of wiping out entire kelp forests on which fish and seals depend. Keystone species illustrate the complex webs of interactions that underpin biodiversity. Understanding, let alone predicting, the impact that removing one species can have on the rest of a non-linear system is devilishly complicated. Even if sensors and ecologists could log the identity and location of every living creature on the planet, such data would be worth little without an understanding of how everything relates to everything else. Computer models are ideally suited to providing just that. General circulation models, for example, simulate the planetary climate, linking the physics that govern the formation and disintegration of ice sheets to the huge currents that push water through the ocean, and oceanic temperature gradients to the formation of storm systems over the continents. These models are so complex that they take months to run, even on the world’s most powerful supercomputers. Climate science and policy would be nothing without them. Ecology has few equivalents. One reason is that ecosystems are much harder to simulate. “In a physical system, you have a set of atoms or molecules that behave in a predictable way, even if it is complex,” says Derek Tittensor, a marine-ecosystem modeller at Dalhousie University in Canada. Ecology, by contrast, deals in living things, whose interactions are determined by the unpredictable behaviour of individuals. Added to this is the complexity of the pressures and stresses that modify ecosystems. Carbon dioxide and methane are produced by different processes and behave differently in the atmosphere, but fundamentally they both warm the atmosphere. Burning fossil fuels also produces a mix of particles which cool the climate. These emissions are all very different, but their effects can, to some approximation, be reduced to a single variable known as their “global-warming potential”. Ecosystems, by contrast, are affected by warming temperatures and changing water cycles, but also by chemical pollution, urban encroachment, hunting and overfishing. None of this can be reduced to just one or even a handful of quantitative variables. And so ecosystem modelling remains in its infancy. Statistical models, built on relationships between historical data sets—for example, how the amount of vegetation in a tropical forest tends to grow or shrink as temperatures and rainfall vary—are easier to build, and have progressed furthest. But they cannot capture or predict the dynamic, non-linear ways ecosystems respond to change, including the tipping points at which cumulative damage to an ecosystem suddenly shifts it into a new regime, for example when deforestation tips a region from forest to savannah. Doing that requires “process-based” or “mechanistic” models, which are harder to build, but can produce non-linearity and emergent behaviour. They are the ecological equivalent of general circulation models, and operate as fully functioning simulations of Earth’s biosphere. They are particularly useful for unpicking what is driving change in an ecosystem. If a fish population is growing, is it because rising temperatures have driven predators away, or because deforestation on land nearby is releasing iron-rich dust which is fertilising the local plankton population? Marine science has produced a number of process-based models, though they are less uniform in their design than climate models. Some are built around food chains and the way they move biomass and energy around ecosystems; others focus on how well-suited different species are to particular ecological niches, or group species and their interactions based on body size, which is a reasonable predictor of an organism’s place in the food chain. Over the past decade marine-ecosystem modellers have formed the Fisheries and Marine Ecosystem Model Intercomparison Project. Its goal is to determine how fishing and climate change are likely to alter marine fisheries around the world, which provide 11% of the animal protein humans consume. “Fish-MIP” develops standardised scenarios that can be run across global and regional marine-ecosystem models. As with climate modelling, the idea is to run the same simulations on different models and combine the results into robust projections that can inform policy decisions. Fish-MIP studies suggest that larger fish species, which make up most of what humans consume, are affected most by climate change, as are the tropics, where people tend to be more dependent on catches and more vulnerable to economic instability and poor nutrition. But simulating the effects of fishing operations is more complicated than studying the impact of rising temperatures, as assumptions have to be made about a range of variables, from how the industry will redistribute fishing fleets as fish migrate towards the poles, to how fishing technology will change, and whether changing attitudes towards sustainability will mean more marine protected areas. The climate-modelling community handles such uncertainty by drawing up standardised hypothetical scenarios and producing climate projections for each one. But the scenarios do not yet take into account the ways in which humans effect biodiversity, such as by overfishing. Modelling is far less advanced for land ecosystems. “Dynamic global vegetation models” can simulate human impacts on plants but do not represent non-human animals. And though there are at least eight global marine-ecosystem models that simulate life in the ocean, there is just one process-based model that includes life on land: the Madingley model, first published in 2014, which represents life both on land and in the ocean. Named after the village in Britain where it was devised, it breaks down land and ocean into grid cells that are up to 200 square km (77 square miles). Climatic conditions are set for each cell, which are also populated with organisms, so long as they weigh more than ten micrograms. To simplify the equations involved, the model groups organisms by size, habitat and function. It therefore cannot distinguish between two species of small songbird that live in the same region, but it does simulate interactions between, say, megafauna and their prey. All this allows for  in silico experiments in which all the world’s top predators are wiped out entirely, an extension in space of Dr Paine’s famous seastar experiment but also an extrapolation of current global trends. An assessment in 2014 of 31 of the world’s largest mammalian carnivores found that three-quarters of them were in decline, and 17 occupied less than half of their historical territory. Using the Mading ley model, Selwyn Hoeks at Radboud University in the Netherlands, and his colleagues found that removing all carnivores weighing more than 21kg triggered a domino effect in food chains with the net result that the total amount of vegetation on Earth decreased. Their results were published in 2020 in the journal Ecography. Ecologists have long argued that conserving large carnivores has tangible benefits beyond the cuddly feeling of saving tigers. According to the “green Earth hypothesis”, no carnivores means more herbivores and thus fewer plants. Vegetation soaks up carbon dioxide, so less plant life would amplify global warming. What of the reverse, where all plant life is gradually removed? Changing landscapes, particularly through agriculture, is humanity’s greatest impact on biodiversity, and one that is likely to increase. Expanding agriculture reduces the amount of plant life at the base of food webs. Tim Newbold, of University College London, and colleagues simulated the removal of increasing amounts of vegetation from China, France, Libya and Uganda. They found that once 80% of plant life was gone, entire food chains began to collapse and could not be rebuilt by simply restoring the plants. As well as predicting outcomes, global ecosystem models make it possible to test policies. What would be the consequence of reintroducing a species from a population bred in captivity? Would the decline of a species be halted or reversed if a percentage of its territorial range were protected, or would it be more efficient to create a corridor between two existing protected areas? Carbon storage, clean water, clean air, abundant crops and fish are all examples of “ecosystem services” that benefit humanity. The principle is undeniable on a grand scale, but the details are harder to map. “We don’t have any frameworks which link biodiversity changes to changes in ecosystem functioning, and on to the services that humans derive from those ecosystems,” says Michael Harfoot of the UN World Conservation Monitoring Centre and co-author of the Ecography paper. Statistical models try to infer changes in ecosystem services from, for instance, trends in forest cover. But process-based models need further refinement so that changes in temperatures or land use can be linked to changes in biodiversity—and then, in turn, to the functioning of ecosystems and the services they provide. “That is probably the next big frontier for ecosystem modelling,” says Dr Harfoot, “and essentially, also, for conservation.” For now, this remains some way off. Today’s ecosystem models are widely compared to where climate models were in their earliest days of development, about 50 years ago. “Given the urgency of the situation, we need ecosystem models to be where climate models will be in ten years’ time,” says Dr Newbold. ■ Full contents of this Technology QuarterlyThe other environmental emergency: Loss of biodiversity poses as great a risk to humanity as climate changeSensors and sensibility: All kinds of new technology are being used to monitor the natural worldCracking the code: The sequencing of genetic material is a powerful conservation toolCrowdsourced science: How volunteer observers can help protect biodiversity* Simulating everything: Compared with climate, modelling of ecosystems is at an early stageBack from the dead: Reviving extinct species may soon be possibleBridging the gap: Technology can help conserve biodiversity